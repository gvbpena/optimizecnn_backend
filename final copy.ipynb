{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    try:\n",
    "        # Load MP3 file and convert to WAV\n",
    "        audio = AudioSegment.from_mp3(file_path)\n",
    "        audio = audio.set_channels(1)  # Convert stereo to mono\n",
    "        audio.export(\"temp.wav\", format=\"wav\")\n",
    "        audio, _ = librosa.load(\"temp.wav\", res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=22050)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=22050)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=22050)\n",
    "        features = np.vstack([mfccs, chroma, spectral_contrast, tonnetz])\n",
    "        mean_features = np.mean(features.T, axis=0)\n",
    "        return mean_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Load Data\n",
    "data = []\n",
    "labels = []\n",
    "genres = os.listdir('./Data/genres_original')\n",
    "# Extract features and labels\n",
    "for genre in genres:\n",
    "    genre_path = os.path.join('./Data/genres_original', genre)\n",
    "    for file in os.listdir(genre_path):\n",
    "        file_path = os.path.join(genre_path, file)\n",
    "        feature = extract_features(file_path)\n",
    "        if feature is not None:\n",
    "            data.append(feature)\n",
    "            labels.append(genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_dict = {label: idx for idx, label in enumerate(set(labels))}\n",
    "numeric_labels = np.array([label_dict[label] for label in labels])\n",
    "# Convert data and labels to numpy arrays\n",
    "X = np.array(data)\n",
    "y = np.array(numeric_labels)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Reshape data for CNN input\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "with open('label_dict.json', 'w') as json_file:\n",
    "    json.dump(label_dict, json_file)\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('X_test_cnn.npy', X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\backend.py:6523: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\optimizers\\__init__.py:300: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\utils\\tf_utils.py:490: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\keras\\engine\\base_layer_utils.py:380: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "13/13 [==============================] - 3s 60ms/step - loss: 5.9593 - accuracy: 0.2175 - val_loss: 4.8368 - val_accuracy: 0.2850\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 4.4328 - accuracy: 0.3363 - val_loss: 4.0581 - val_accuracy: 0.3000\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 3.7462 - accuracy: 0.3537 - val_loss: 3.4913 - val_accuracy: 0.3350\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 3.2457 - accuracy: 0.4300 - val_loss: 3.2198 - val_accuracy: 0.3350\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 2.8977 - accuracy: 0.4412 - val_loss: 2.8424 - val_accuracy: 0.3950\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2.6297 - accuracy: 0.4975 - val_loss: 2.6361 - val_accuracy: 0.4250\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2.4596 - accuracy: 0.4825 - val_loss: 2.4530 - val_accuracy: 0.4850\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2.3022 - accuracy: 0.5175 - val_loss: 2.3564 - val_accuracy: 0.4900\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2.1462 - accuracy: 0.5263 - val_loss: 2.2570 - val_accuracy: 0.4550\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 2.0185 - accuracy: 0.5663 - val_loss: 2.1085 - val_accuracy: 0.5500\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.9720 - accuracy: 0.5587 - val_loss: 2.0920 - val_accuracy: 0.5100\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.8925 - accuracy: 0.5650 - val_loss: 1.9989 - val_accuracy: 0.5250\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.7916 - accuracy: 0.5663 - val_loss: 1.9561 - val_accuracy: 0.4950\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 17ms/step - loss: 1.6959 - accuracy: 0.6037 - val_loss: 1.8958 - val_accuracy: 0.5450\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.6623 - accuracy: 0.5962 - val_loss: 1.8586 - val_accuracy: 0.5550\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 1.5888 - accuracy: 0.6237 - val_loss: 1.7886 - val_accuracy: 0.5350\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.5157 - accuracy: 0.6313 - val_loss: 1.8574 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.4334 - accuracy: 0.6712 - val_loss: 1.7568 - val_accuracy: 0.5350\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.4256 - accuracy: 0.6413 - val_loss: 1.7376 - val_accuracy: 0.5400\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 20ms/step - loss: 1.3927 - accuracy: 0.6562 - val_loss: 1.6600 - val_accuracy: 0.5500\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.3363 - accuracy: 0.6575 - val_loss: 1.6868 - val_accuracy: 0.5300\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.3052 - accuracy: 0.6812 - val_loss: 1.5997 - val_accuracy: 0.5800\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.2629 - accuracy: 0.6787 - val_loss: 1.6229 - val_accuracy: 0.5950\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.1937 - accuracy: 0.7050 - val_loss: 1.5708 - val_accuracy: 0.6050\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.1593 - accuracy: 0.7250 - val_loss: 1.5703 - val_accuracy: 0.5750\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.1038 - accuracy: 0.7250 - val_loss: 1.6271 - val_accuracy: 0.5600\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.0850 - accuracy: 0.7312 - val_loss: 1.4929 - val_accuracy: 0.6050\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.0205 - accuracy: 0.7563 - val_loss: 1.5036 - val_accuracy: 0.5950\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.0203 - accuracy: 0.7312 - val_loss: 1.4682 - val_accuracy: 0.6050\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 1.0019 - accuracy: 0.7337 - val_loss: 1.4630 - val_accuracy: 0.5800\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.9811 - accuracy: 0.7538 - val_loss: 1.5160 - val_accuracy: 0.5800\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.9503 - accuracy: 0.7700 - val_loss: 1.5402 - val_accuracy: 0.5750\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.9304 - accuracy: 0.7725 - val_loss: 1.4525 - val_accuracy: 0.6100\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.9092 - accuracy: 0.7887 - val_loss: 1.5260 - val_accuracy: 0.5700\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8280 - accuracy: 0.8125 - val_loss: 1.5590 - val_accuracy: 0.6100\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8099 - accuracy: 0.8087 - val_loss: 1.4254 - val_accuracy: 0.6550\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.8359 - accuracy: 0.7975 - val_loss: 1.5644 - val_accuracy: 0.6050\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7863 - accuracy: 0.8300 - val_loss: 1.4124 - val_accuracy: 0.6350\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7374 - accuracy: 0.8238 - val_loss: 1.3876 - val_accuracy: 0.6650\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7696 - accuracy: 0.8225 - val_loss: 1.4271 - val_accuracy: 0.6450\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7455 - accuracy: 0.8250 - val_loss: 1.4598 - val_accuracy: 0.6150\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6910 - accuracy: 0.8438 - val_loss: 1.5130 - val_accuracy: 0.6150\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7039 - accuracy: 0.8438 - val_loss: 1.6077 - val_accuracy: 0.6100\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.7037 - accuracy: 0.8388 - val_loss: 1.4705 - val_accuracy: 0.6100\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.6554 - accuracy: 0.8475 - val_loss: 1.4005 - val_accuracy: 0.6450\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6490 - accuracy: 0.8363 - val_loss: 1.4926 - val_accuracy: 0.6250\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.6252 - accuracy: 0.8625 - val_loss: 1.4640 - val_accuracy: 0.6400\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5965 - accuracy: 0.8788 - val_loss: 1.4839 - val_accuracy: 0.6300\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.5881 - accuracy: 0.8625 - val_loss: 1.6074 - val_accuracy: 0.5950\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 15ms/step - loss: 0.5635 - accuracy: 0.8725 - val_loss: 1.4623 - val_accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "# Build Improved CNN Model\n",
    "def build_cnn_model(input_shape, filters=64, kernel_size=3, dropout_rate=0.5, l2_reg=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters*2, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters*4, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(label_dict), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "improved_cnn_model = build_cnn_model(input_shape=(X_train_cnn.shape[1], 1))\n",
    "improved_cnn_model.fit(X_train_cnn, y_train_onehot, epochs=50, batch_size=64, validation_data=(X_test_cnn, y_test_onehot))\n",
    "\n",
    "improved_cnn_model.save('improved_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized SVM Model Accuracy: 0.615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optimized_svm_model.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Classifier with Grid Search\n",
    "svm_param_grid = {'C': [0.1, 1, 10, 100],\n",
    "                  'kernel': ['linear', 'rbf', 'poly'],\n",
    "                  'gamma': ['scale', 'auto']}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=3)\n",
    "svm_grid_search.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "best_svm_params = svm_grid_search.best_params_\n",
    "svm_classifier_optimized = SVC(**best_svm_params)\n",
    "svm_classifier_optimized.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Make predictions with optimized SVM\n",
    "svm_predictions_optimized = svm_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "svm_accuracy_optimized = accuracy_score(y_test, svm_predictions_optimized)\n",
    "print(f\"Optimized SVM Model Accuracy: {svm_accuracy_optimized}\")\n",
    "joblib.dump(svm_classifier_optimized, 'optimized_svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest Model Accuracy: 0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['optimized_rf_model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier with Grid Search\n",
    "rf_param_grid = {'n_estimators': [50, 100, 150],\n",
    "                 'max_depth': [None, 10, 20, 30],\n",
    "                 'min_samples_split': [2, 5, 10],\n",
    "                 'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3)\n",
    "rf_grid_search.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "rf_classifier_optimized = RandomForestClassifier(**best_rf_params)\n",
    "rf_classifier_optimized.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Make predictions with optimized Random Forest\n",
    "rf_predictions_optimized = rf_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "rf_accuracy_optimized = accuracy_score(y_test, rf_predictions_optimized)\n",
    "print(f\"Optimized Random Forest Model Accuracy: {rf_accuracy_optimized}\")\n",
    "joblib.dump(rf_classifier_optimized, 'optimized_rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n",
      "Optimized Ensemble Model Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "# ...\n",
    "\n",
    "# Load models\n",
    "svm_classifier_optimized = joblib.load('optimized_svm_model.joblib')\n",
    "rf_classifier_optimized = joblib.load('optimized_rf_model.joblib')\n",
    "improved_cnn_model = load_model('improved_cnn_model.h5')\n",
    "\n",
    "# Make predictions with the individual models\n",
    "svm_predictions_optimized = svm_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "rf_predictions_optimized = rf_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "cnn_predictions_optimized_probs = improved_cnn_model.predict(X_test_cnn)\n",
    "\n",
    "# Extract the predicted classes from the probabilities\n",
    "cnn_predictions_optimized = np.argmax(cnn_predictions_optimized_probs, axis=1)\n",
    "\n",
    "# Stack the predictions vertically\n",
    "ensemble_predictions_optimized = np.vstack([svm_predictions_optimized, rf_predictions_optimized, cnn_predictions_optimized])\n",
    "\n",
    "\n",
    "# Experiment with adjusting weights for each model in majority voting\n",
    "weights = [2, 1, 1]  # Adjust these weights based on individual model performance\n",
    "\n",
    "# Use weighted majority voting to determine the final prediction\n",
    "weighted_majority_voting_predictions_optimized = np.apply_along_axis(\n",
    "    lambda x: np.argmax(np.bincount(x, weights=weights)), axis=0, arr=ensemble_predictions_optimized)\n",
    "\n",
    "# Evaluate the ensemble accuracy\n",
    "ensemble_accuracy_optimized = accuracy_score(y_test, weighted_majority_voting_predictions_optimized)\n",
    "print(f\"Optimized Ensemble Model Accuracy: {ensemble_accuracy_optimized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "Shape of extracted features: (38,)\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "{\n",
      "  \"predicted_genre\": \"disco\",\n",
      "  \"predicted_percentages\": {\n",
      "    \"blues\": 20.056311786174774,\n",
      "    \"hiphop\": 5.588407814502716,\n",
      "    \"country\": 0.14554032823070884,\n",
      "    \"rock\": 0.2600289648398757,\n",
      "    \"pop\": 0.6814057938754559,\n",
      "    \"jazz\": 1.336020976305008,\n",
      "    \"metal\": 0.014798181655351073,\n",
      "    \"disco\": 70.73978185653687,\n",
      "    \"reggae\": 0.7425688672810793,\n",
      "    \"classical\": 0.4351396579295397\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"predicted_genre\": \"disco\",\n",
      "  \"predicted_percentages\": {\n",
      "    \"disco\": 84.56629180908203,\n",
      "    \"hiphop\": 9.314011573791504,\n",
      "    \"jazz\": 2.226701259613037,\n",
      "    \"reggae\": 1.237614631652832,\n",
      "    \"pop\": 1.1356761455535889,\n",
      "    \"classical\": 0.7252326011657715,\n",
      "    \"rock\": 0.4333815574645996,\n",
      "    \"country\": 0.24256718158721924,\n",
      "    \"blues\": 0.09384933859109879,\n",
      "    \"metal\": 0.024663634598255157\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_features(file_path):\n",
    "    try:\n",
    "        # Load MP3 file and convert to WAV\n",
    "        audio = AudioSegment.from_mp3(file_path)\n",
    "        audio = audio.set_channels(1)  # Convert stereo to mono\n",
    "        audio.export(\"temp.wav\", format=\"wav\")\n",
    "        audio, _ = librosa.load(\"temp.wav\", res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=22050)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=22050)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=22050)\n",
    "        features = np.vstack([mfccs, chroma, spectral_contrast, tonnetz])\n",
    "        mean_features = np.mean(features.T, axis=0)\n",
    "        return mean_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file '{file_path}': {e}\")\n",
    "        return None\n",
    "# Load label_dict\n",
    "with open('label_dict.json', 'r') as json_file:\n",
    "    label_dict = json.load(json_file)\n",
    "\n",
    "# Load X_test\n",
    "X_test = np.load('X_test.npy')\n",
    "X_test_cnn = np.load('X_test_cnn.npy')\n",
    "# Load models\n",
    "def optimize_cnn_model(testing_feature, svm_classifier_optimized, rf_classifier_optimized, improved_cnn_model, normalized_weights, label_dict):\n",
    "    result_dict = {}\n",
    "    # Reshape features for CNN input\n",
    "    testing_feature_cnn = testing_feature.reshape(1, testing_feature.shape[0], 1)\n",
    "    # Make predictions with the individual models\n",
    "    svm_prediction = svm_classifier_optimized.predict(testing_feature.reshape(1, -1))\n",
    "    rf_prediction = rf_classifier_optimized.predict(testing_feature.reshape(1, -1))\n",
    "    cnn_prediction_probs = improved_cnn_model.predict(testing_feature_cnn)\n",
    "    cnn_prediction = np.argmax(cnn_prediction_probs, axis=1)\n",
    "    # Ensemble: Weighted Voting with normalized weights\n",
    "    ensemble_prediction_probs = (\n",
    "        normalized_weights[0] * to_categorical(svm_prediction, num_classes=len(label_dict)) +\n",
    "        normalized_weights[1] * to_categorical(rf_prediction, num_classes=len(label_dict)) +\n",
    "        normalized_weights[2] * cnn_prediction_probs\n",
    "    )\n",
    "    # Normalize ensemble predictions to ensure they sum up to 1\n",
    "    normalized_ensemble_probs = ensemble_prediction_probs / sum(ensemble_prediction_probs[0])\n",
    "    weighted_majority_voting_prediction = np.argmax(normalized_ensemble_probs)\n",
    "    predicted_genre = list(label_dict.keys())[weighted_majority_voting_prediction]\n",
    "    result_dict[\"predicted_genre\"] = predicted_genre\n",
    "    # Store the predicted percentages in the result dictionary\n",
    "    result_dict[\"predicted_percentages\"] = {genre: percentage.item() * 100 for genre, percentage in zip(label_dict.keys(), normalized_ensemble_probs[0])}\n",
    "    return json.dumps(result_dict, indent=2)\n",
    "\n",
    "def evaluate_cnn_model(testing_feature, loaded_model, label_dict):\n",
    "    result_dict = {}\n",
    "    # Check if testing_feature is not None\n",
    "    if testing_feature is not None:\n",
    "        print(f\"Shape of extracted features: {testing_feature.shape}\")\n",
    "        # Reshape features for CNN input\n",
    "        testing_feature_cnn = testing_feature.reshape(1, testing_feature.shape[0], 1)\n",
    "        # Use the model to predict the genre\n",
    "        prediction = loaded_model.predict(testing_feature_cnn)\n",
    "        # Get the predicted percentages for each genre\n",
    "        predicted_percentages = (prediction * 100).tolist()[0]\n",
    "        # Create a list of tuples with genre and its percentage\n",
    "        genre_percentage_list = [(genre, percentage) for genre, percentage in zip(label_dict.keys(), predicted_percentages)]\n",
    "        # Sort the list based on percentage in descending order\n",
    "        genre_percentage_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        # Store the predicted genre and percentage in the result dictionary\n",
    "        result_dict[\"predicted_genre\"] = genre_percentage_list[0][0]\n",
    "        result_dict[\"predicted_percentages\"] = {genre: percentage for genre, percentage in genre_percentage_list}\n",
    "    else:\n",
    "        result_dict[\"error_message\"] = \"Error extracting features from 'testing.wav'\"\n",
    "\n",
    "    return json.dumps(result_dict, indent=2)\n",
    "\n",
    "\n",
    "svm_classifier_optimized = joblib.load('optimized_svm_model.joblib')\n",
    "rf_classifier_optimized = joblib.load('optimized_rf_model.joblib')\n",
    "improved_cnn_model = load_model('improved_cnn_model.h5')\n",
    "\n",
    "# Make predictions with the individual models\n",
    "svm_predictions_optimized = svm_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "rf_predictions_optimized = rf_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "cnn_predictions_optimized_probs = improved_cnn_model.predict(X_test_cnn)\n",
    "cnn_predictions_optimized = np.argmax(cnn_predictions_optimized_probs, axis=1)\n",
    "# Ensemble: Weighted Voting\n",
    "weights = [0.2, 0.2, 0.6]  # Adjust these weights based on individual model performance\n",
    "# Normalize weights to ensure they sum up to 1\n",
    "normalized_weights = np.array(weights) / sum(weights)\n",
    "\n",
    "# Extract features from testing.wav\n",
    "testing_file_path = './testing.wav'  # Replace with the actual path\n",
    "testing_feature = extract_features(testing_file_path)\n",
    "\n",
    "result_optimize_cnn = optimize_cnn_model(testing_feature, svm_classifier_optimized, rf_classifier_optimized, improved_cnn_model, normalized_weights, label_dict)\n",
    "result_cnn = evaluate_cnn_model(testing_feature, improved_cnn_model, label_dict)\n",
    "print(result_optimize_cnn)\n",
    "print(result_cnn)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
