{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Function Definitions\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import librosa\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to extract features from an audio file with more features\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        # Load MP3 file and convert to WAV\n",
    "        audio = AudioSegment.from_mp3(file_path)\n",
    "        audio = audio.set_channels(1)  # Convert stereo to mono\n",
    "        audio.export(\"temp.wav\", format=\"wav\")\n",
    "        audio, _ = librosa.load(\"temp.wav\", res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=22050)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=22050)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=22050)\n",
    "        features = np.vstack([mfccs, chroma, spectral_contrast, tonnetz])\n",
    "        mean_features = np.mean(features.T, axis=0)\n",
    "        return mean_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file '{file_path}': {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(genre_path):\n\u001b[0;32m      9\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(genre_path, file)\n\u001b[1;32m---> 10\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(feature)\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m chroma \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m)\n\u001b[0;32m     27\u001b[0m spectral_contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m tonnetz \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtonnetz\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([mfccs, chroma, spectral_contrast, tonnetz])\n\u001b[0;32m     30\u001b[0m mean_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(features\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\feature\\spectral.py:1808\u001b[0m, in \u001b[0;36mtonnetz\u001b[1;34m(y, sr, chroma, **kwargs)\u001b[0m\n\u001b[0;32m   1802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m   1803\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither the audio samples or the chromagram must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1804\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed as an argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1805\u001b[0m     )\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chroma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1808\u001b[0m     chroma \u001b[38;5;241m=\u001b[39m \u001b[43mchroma_cqt\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;66;03m# Generate Transformation matrix\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m dim_map \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m12\u001b[39m, num\u001b[38;5;241m=\u001b[39mchroma\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\feature\\spectral.py:1386\u001b[0m, in \u001b[0;36mchroma_cqt\u001b[1;34m(y, sr, C, hop_length, fmin, norm, threshold, tuning, n_chroma, n_octaves, window, bins_per_octave, cqt_mode)\u001b[0m\n\u001b[0;32m   1381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m   1383\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of C or y must be provided to compute chroma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m         )\n\u001b[0;32m   1385\u001b[0m     C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[1;32m-> 1386\u001b[0m         \u001b[43mcqt_func\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcqt_mode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m            \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_octaves\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbins_per_octave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbins_per_octave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins_per_octave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtuning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1395\u001b[0m     )\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;66;03m# Map to chroma\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m cq_to_chr \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mcq_to_chroma(\n\u001b[0;32m   1399\u001b[0m     C\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m   1400\u001b[0m     bins_per_octave\u001b[38;5;241m=\u001b[39mbins_per_octave,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1403\u001b[0m     window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[0;32m   1404\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\core\\constantq.py:171\u001b[0m, in \u001b[0;36mcqt\u001b[1;34m(y, sr, hop_length, fmin, n_bins, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the constant-Q transform of an audio signal.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mThis implementation is based on the recursive sub-sampling method\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m       [5.147e-02, 6.959e-02, ..., 1.694e-05, 5.811e-06]])\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# CQT is the special case of VQT with gamma=0\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvqt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mequal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins_per_octave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins_per_octave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\core\\constantq.py:1001\u001b[0m, in \u001b[0;36mvqt\u001b[1;34m(y, sr, hop_length, fmin, n_bins, intervals, gamma, bins_per_octave, tuning, filter_scale, norm, sparsity, window, scale, pad_mode, res_type, dtype)\u001b[0m\n\u001b[0;32m    998\u001b[0m freqs_oct \u001b[38;5;241m=\u001b[39m freqs[sl]\n\u001b[0;32m    999\u001b[0m alpha_oct \u001b[38;5;241m=\u001b[39m alpha[sl]\n\u001b[1;32m-> 1001\u001b[0m fft_basis, n_fft, _ \u001b[38;5;241m=\u001b[39m \u001b[43m__vqt_filter_fft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmy_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreqs_oct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_oct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;66;03m# Re-scale the filters to compensate for downsampling\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m fft_basis[:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(sr \u001b[38;5;241m/\u001b[39m my_sr)\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\core\\constantq.py:1063\u001b[0m, in \u001b[0;36m__vqt_filter_fft\u001b[1;34m(sr, freqs, filter_scale, norm, sparsity, hop_length, window, gamma, dtype, alpha)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;129m@cache\u001b[39m(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__vqt_filter_fft\u001b[39m(\n\u001b[0;32m   1051\u001b[0m     sr,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1061\u001b[0m ):\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate the frequency domain variable-Q filter basis.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m     basis, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavelet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# Filters are padded up to the nearest integral power of 2\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     n_fft \u001b[38;5;241m=\u001b[39m basis\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gvbpena_pc\\Desktop\\thesis\\venv\\Lib\\site-packages\\librosa\\filters.py:978\u001b[0m, in \u001b[0;36mwavelet\u001b[1;34m(freqs, sr, window, filter_scale, pad_fft, norm, dtype, gamma, alpha, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m    976\u001b[0m     sig \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mnormalize(sig, norm\u001b[38;5;241m=\u001b[39mnorm)\n\u001b[1;32m--> 978\u001b[0m     filters\u001b[38;5;241m.\u001b[39mappend(sig)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# Pad and stack\u001b[39;00m\n\u001b[0;32m    981\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lengths)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data\n",
    "data = []\n",
    "labels = []\n",
    "genres = os.listdir('./Data/genres_original')\n",
    "# Extract features and labels\n",
    "for genre in genres:\n",
    "    genre_path = os.path.join('./Data/genres_original', genre)\n",
    "    for file in os.listdir(genre_path):\n",
    "        file_path = os.path.join(genre_path, file)\n",
    "        feature = extract_features(file_path)\n",
    "        if feature is not None:\n",
    "            data.append(feature)\n",
    "            labels.append(genre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_dict = {label: idx for idx, label in enumerate(set(labels))}\n",
    "numeric_labels = np.array([label_dict[label] for label in labels])\n",
    "# Convert data and labels to numpy arrays\n",
    "X = np.array(data)\n",
    "y = np.array(numeric_labels)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Reshape data for CNN input\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build Improved CNN Model\n",
    "def build_cnn_model(input_shape, filters=64, kernel_size=3, dropout_rate=0.5, l2_reg=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters*2, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters*4, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(label_dict), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "improved_cnn_model = build_cnn_model(input_shape=(X_train_cnn.shape[1], 1))\n",
    "improved_cnn_model.fit(X_train_cnn, y_train_onehot, epochs=50, batch_size=64, validation_data=(X_test_cnn, y_test_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "improved_cnn_model.save('improved_cnn_model.h5')\n",
    "improved_cnn_predictions = np.argmax(improved_cnn_model.predict(X_test_cnn), axis=1)\n",
    "loaded_model = load_model('improved_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Extract features from testing.wav\n",
    "testing_file_path = './testing.wav'  # Replace with the actual path\n",
    "testing_feature = extract_features(testing_file_path)\n",
    "improved_cnn_accuracy = accuracy_score(y_test, improved_cnn_predictions)\n",
    "print(f\"CNN Model Accuracy: {improved_cnn_accuracy}\")\n",
    "\n",
    "if testing_feature is not None:\n",
    "    print(f\"Shape of extracted features: {testing_feature.shape}\")\n",
    "    # Reshape features for CNN input\n",
    "    testing_feature_cnn = testing_feature.reshape(1, testing_feature.shape[0], 1)\n",
    "    # Use the model to predict the genre\n",
    "    prediction = loaded_model.predict(testing_feature_cnn)\n",
    "    # Get the predicted percentages for each genre\n",
    "    predicted_percentages = (prediction * 100).tolist()[0]\n",
    "    # Create a list of tuples with genre and its percentage\n",
    "    genre_percentage_list = [(genre, percentage) for genre, percentage in zip(label_dict.keys(), predicted_percentages)]\n",
    "    # Sort the list based on percentage in descending order\n",
    "    genre_percentage_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Print the predicted genre and percentage\n",
    "    for genre, percentage in genre_percentage_list:\n",
    "        print(f\"The predicted percentage for '{genre}' is: {percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Error extracting features from 'testing.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.externals import joblib\n",
    "# Define the SVM model\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "# Define the parameter grid for grid search\n",
    "svm_param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "# Perform grid search\n",
    "svm_grid_search = GridSearchCV(svm_model, svm_param_grid, cv=3)\n",
    "svm_grid_search.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "# Get the best parameters from grid search\n",
    "best_svm_params = svm_grid_search.best_params_\n",
    "# Train SVM with best parameters\n",
    "svm_classifier_optimized = svm.SVC(kernel='linear', C=best_svm_params['C'])\n",
    "svm_classifier_optimized.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Make predictions with optimized SVM\n",
    "svm_predictions_optimized = svm_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "svm_accuracy_optimized = accuracy_score(y_test, svm_predictions_optimized)\n",
    "print(f\"Optimized SVM Model Accuracy: {svm_accuracy_optimized}\")\n",
    "# joblib.dump(svm_classifier_optimized, 'optimized_svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(svm_classifier_optimized, 'optimized_svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "rf_param_grid = {'n_estimators': [50, 100, 150],\n",
    "                 'max_depth': [None, 10, 20, 30]}\n",
    "\n",
    "# Perform grid search\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=3)\n",
    "rf_grid_search.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Get the best parameters from grid search\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "\n",
    "# Train Random Forest with best parameters\n",
    "rf_classifier_optimized = RandomForestClassifier(n_estimators=best_rf_params['n_estimators'],\n",
    "                                                 max_depth=best_rf_params['max_depth'],\n",
    "                                                 random_state=42)\n",
    "rf_classifier_optimized.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "# Make predictions with optimized Random Forest\n",
    "rf_predictions_optimized = rf_classifier_optimized.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "rf_accuracy_optimized = accuracy_score(y_test, rf_predictions_optimized)\n",
    "print(f\"Optimized Random Forest Model Accuracy: {rf_accuracy_optimized}\")\n",
    "\n",
    "import joblib\n",
    "joblib.dump(rf_classifier_optimized, 'optimized_rf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the optimized SVM model\n",
    "svm_model = joblib.load('optimized_svm_model.joblib')\n",
    "\n",
    "# Load the optimized Random Forest model\n",
    "rf_model = joblib.load('optimized_rf_model.joblib')\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "cnn_model = load_model('improved_cnn_model.h5')\n",
    "\n",
    "# Make predictions with optimized SVM and Random Forest\n",
    "svm_predictions_optimized = svm_model.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "rf_predictions_optimized = rf_model.predict(X_test)\n",
    "\n",
    "# Make predictions with the improved CNN model\n",
    "improved_cnn_predictions = cnn_model.predict(X_test)\n",
    "\n",
    "# Convert CNN predictions to labels (assuming it's a multi-class classification problem)\n",
    "improved_cnn_predictions_labels = np.argmax(improved_cnn_predictions, axis=1)\n",
    "\n",
    "# Stack the predictions vertically\n",
    "ensemble_predictions_optimized = np.vstack([svm_predictions_optimized, rf_predictions_optimized, improved_cnn_predictions_labels])\n",
    "\n",
    "# Use majority voting to determine the final prediction\n",
    "majority_voting_predictions_optimized = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=ensemble_predictions_optimized)\n",
    "\n",
    "# Evaluate the ensemble accuracy\n",
    "ensemble_accuracy_optimized = accuracy_score(y_test, majority_voting_predictions_optimized)\n",
    "print(f\"Optimized Ensemble Model Accuracy: {ensemble_accuracy_optimized}\")\n",
    "\n",
    "joblib.dump(ensemble_predictions_optimized, 'ensemble_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ensemble model predictions\n",
    "ensemble_predictions_optimized = joblib.load('ensemble_model.joblib')\n",
    "\n",
    "# Use majority voting to determine the final prediction\n",
    "majority_voting_predictions_optimized = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=ensemble_predictions_optimized)\n",
    "\n",
    "# Evaluate the ensemble accuracy\n",
    "ensemble_accuracy_optimized = accuracy_score(y_test, majority_voting_predictions_optimized)\n",
    "print(f\"Optimized Ensemble Model Accuracy: {ensemble_accuracy_optimized}\")\n",
    "\n",
    "# Assuming 'ensemble_classifier' is the trained majority voting ensemble\n",
    "# You should have it already trained from the previous code\n",
    "\n",
    "# Cell 6: Extract features from testing.wav\n",
    "testing_file_path = './testing.wav'  # Replace with the actual path\n",
    "testing_feature = extract_features(testing_file_path)  # Replace with your feature extraction code\n",
    "\n",
    "# ...\n",
    "\n",
    "if testing_feature is not None:\n",
    "    print(f\"Shape of extracted features: {testing_feature.shape}\")\n",
    "\n",
    "    # Reshape features for CNN input (assuming the CNN model is the same as used in the ensemble)\n",
    "    testing_feature_cnn = testing_feature.reshape(1, testing_feature.shape[0], 1)\n",
    "\n",
    "    # Use the ensemble model predictions to get the final prediction\n",
    "    ensemble_prediction_index = majority_voting_predictions_optimized[0]\n",
    "    \n",
    "    # Get the corresponding genre label from the numeric label\n",
    "    predicted_genre = list(label_dict.keys())[ensemble_prediction_index]\n",
    "    print(f\"The predicted genre is: {predicted_genre}\")\n",
    "\n",
    "    # Calculate the percentage of each genre in the ensemble predictions\n",
    "    unique_genres, counts = np.unique(majority_voting_predictions_optimized, return_counts=True)\n",
    "    total_predictions = len(majority_voting_predictions_optimized)\n",
    "\n",
    "    for genre_index, count in zip(unique_genres, counts):\n",
    "        genre_label = list(label_dict.keys())[genre_index]\n",
    "        percentage_genre = (count / total_predictions) * 100\n",
    "        print(f\"The predicted percentage for '{genre_label}' is: {percentage_genre:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Error extracting features from 'testing.wav'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
